{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nearest_Neighbors",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/lylemp/bwsi_testsets/blob/master/Nearest_Neighbors.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "UgLMQPlIX9_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Nearest Neighbors Algorithm for Supervised Learning\n",
        "\n",
        "\n",
        "Nearest neighbors algorithms (NNAs) are very simple conceptually: to classify a datum with specific feature values, find the data point that has the most similar feature values and put the original datum in that class. NNAs can also be used to predict missing feature values. \n",
        "\n",
        "The most common NNA is the k-Nearest Neighbors algorithm where the top *K* nearest neighbors to the query are identified. In most instantiations of k-NNA, classification/prediction is based on a \"majority vote\" of the k nearest neighbors (ex. if k = 5 and 3 out of the 5 nearest neighbors are class A and 2 are class B, the new data point will be classified as A). In the image below, using k = 1 would yield a class 1 classification while k = 3 would yield class 2.\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*Sk18h9op6uK9EpT8.)\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "A quick example to illustrate k-NNA:\n",
        "\n",
        "If we want to classify a car as \"cool\" or \"uncool\" using the features *horsepower, number of seats,* and *manual (0) or automatic (1)*, our dataset might look like this:\n",
        "\n",
        "*   150, 5, 0, uncool (2008 Honda Civic)  \n",
        "*   320, 5, 0, cool (2011 Dodge Charger)\n",
        "*   383, 3, 1, cool (1985 Chevy Blazer)\n",
        "*   210, 7, 0, uncool (2001 Honda Odyssey)\n",
        "\n",
        "Let's say we're trying to predict whether the 2017 Bugatti Veyron (1500hp, 2 seats, manual: 1) is cool or not. In the next cell the data is loaded. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9GQ247sLX_gv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "11bd6a26-088c-4a8d-8dab-6960ea422a13"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "import random\n",
        "from numpy.random import permutation\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "data = pd.DataFrame({'2008 Honda Civic': [150, 5, 0, 0], '2011 Dodge Charger' : [320, 5, 0, 1],  # 1 = cool, 0 = uncool (4th item in list)\n",
        "       '1985 Chevy Blazer': [383, 3, 1, 1], '2001 Honda Odyssey': [210, 7, 0,0], '2017 Bugatti Veyron': [1500, 2, 1, None]})\n",
        "\n",
        "data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1985 Chevy Blazer</th>\n",
              "      <th>2001 Honda Odyssey</th>\n",
              "      <th>2008 Honda Civic</th>\n",
              "      <th>2011 Dodge Charger</th>\n",
              "      <th>2017 Bugatti Veyron</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>383</td>\n",
              "      <td>210</td>\n",
              "      <td>150</td>\n",
              "      <td>320</td>\n",
              "      <td>1500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   1985 Chevy Blazer  2001 Honda Odyssey  2008 Honda Civic  \\\n",
              "0                383                 210               150   \n",
              "1                  3                   7                 5   \n",
              "2                  1                   0                 0   \n",
              "3                  1                   0                 0   \n",
              "\n",
              "   2011 Dodge Charger  2017 Bugatti Veyron  \n",
              "0                 320               1500.0  \n",
              "1                   5                  2.0  \n",
              "2                   0                  1.0  \n",
              "3                   1                  NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "RmXWYA6zYDRH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalizing Data and Calculating Distance\n",
        "\n",
        "In order to tell how \"near\" one datum is to another, we need a way to measure the distance between two data. One of the simplest ways to do this (and the way we will be doing it) is with simple Euclidian distance (formula below). Euclidean distance is the square root of the sum of the difference between each feature squared. Some other approaches include Chi square distance and cosine distance (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/). \n",
        "\n",
        "NOTE: By normalizing the data in this way we are assuming all features are equally important, but there are ways to weight some features more/less than others.\n",
        "\n",
        "If we simply compute the Euclidean distance from one datum to another, values that are generally larger (like horsepower) will end up having a greater effect than the other smaller values. Because features with large values are not inherently more important for prediction than other values, we should normalize the data before we calculate distance. One quick and easy way to normalize data is to divide each datum by the maximum value in its category (i.e. divide each element in a row by the max value in that row).\n",
        "\n",
        "![alt text](https://i.stack.imgur.com/2y0bx.png.)\n",
        "\n",
        "In the cells below, fill in the code to normalize the data and create a function that will calculate the euclidean distance between two cars. "
      ]
    },
    {
      "metadata": {
        "id": "DLlQMA8mYDzf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalization \n",
        "\n",
        "## Normalize the data below by dividing each value by the maximum value in its row. Do not normalize the labels indicating cool and uncool (row 3)\n",
        "\n",
        "for i in range(3):\n",
        "  ## Your code here\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FZEbFVS9YIn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Distance Function using formula for euclidean distance\n",
        "\n",
        "def euclidean_dist(new_datum, other_datum):\n",
        "  \n",
        "  ## Your code here\n",
        "  \n",
        "  return(distance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_6FSriiYHC-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the cell below, calculate the distance between the Bugatti and each other car using euclidean_dist. Remember not to use row three (cool/uncool label) when computing distance."
      ]
    },
    {
      "metadata": {
        "id": "136t0YzIYNBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NW9DpEWaYPPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should get the following distances (rounded to the thousandths palce):\n",
        "\n",
        "Bugatti - Blazer = 0.758\n",
        "\n",
        "Bugatti - Odyssey = 1.500\n",
        "\n",
        "Bugatti - Civic = 1.412\n",
        "\n",
        "Bugatti - Charger = 1.343\n",
        "\n",
        "Because the distance between the Bugatti and Blazer is the smallest, if k = 1, we would classify the Bugatti as cool. If k = 4 we would not be able to classify the Bugatti in either category using the \"majority vote\" technique unless we had in place a tiebreaker protocol. Generally speaking, larger values of *k* reduce noise, but also make the boundaries between classes less distinct. The best value of *k* will depend on your dataset and your prediction needs."
      ]
    },
    {
      "metadata": {
        "id": "SFcduIhDYRpv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predicting Diabetes in Pima Heritage Dataset\n",
        "\n",
        "Next we will see if we can use k-NNA to predict whether or not a patient has diabetes given some medical information about them. Load and view the data in the cell below. \n"
      ]
    },
    {
      "metadata": {
        "id": "R1UQwYOLYTjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "17048f00-c84e-4d2d-cb00-16d548a05d6a"
      },
      "cell_type": "code",
      "source": [
        "# Data Loading and preprocessing\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "#  'preg': number of pregnancies  \n",
        "#  'plas': plasma glucose concentration \n",
        "#  'pres': blood pressure \n",
        "#  'skin': skin thickness\n",
        "#  'test': Insulin\n",
        "#  'mass': BMI\n",
        "#  'pedi': diabetes pedigree function\n",
        "#  'age': age\n",
        "#  'class': '0' means does not have diabetes and '1' means has diabetes\n",
        "\n",
        "data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>test</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>116</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.6</td>\n",
              "      <td>0.201</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>78</td>\n",
              "      <td>50</td>\n",
              "      <td>32</td>\n",
              "      <td>88</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.248</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.3</td>\n",
              "      <td>0.134</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>197</td>\n",
              "      <td>70</td>\n",
              "      <td>45</td>\n",
              "      <td>543</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.158</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>125</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.232</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>110</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.6</td>\n",
              "      <td>0.191</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>168</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.537</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>10</td>\n",
              "      <td>139</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.1</td>\n",
              "      <td>1.441</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>189</td>\n",
              "      <td>60</td>\n",
              "      <td>23</td>\n",
              "      <td>846</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.398</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>166</td>\n",
              "      <td>72</td>\n",
              "      <td>19</td>\n",
              "      <td>175</td>\n",
              "      <td>25.8</td>\n",
              "      <td>0.587</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>7</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.484</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>118</td>\n",
              "      <td>84</td>\n",
              "      <td>47</td>\n",
              "      <td>230</td>\n",
              "      <td>45.8</td>\n",
              "      <td>0.551</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>7</td>\n",
              "      <td>107</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29.6</td>\n",
              "      <td>0.254</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>103</td>\n",
              "      <td>30</td>\n",
              "      <td>38</td>\n",
              "      <td>83</td>\n",
              "      <td>43.3</td>\n",
              "      <td>0.183</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>115</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>96</td>\n",
              "      <td>34.6</td>\n",
              "      <td>0.529</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>126</td>\n",
              "      <td>88</td>\n",
              "      <td>41</td>\n",
              "      <td>235</td>\n",
              "      <td>39.3</td>\n",
              "      <td>0.704</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8</td>\n",
              "      <td>99</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.4</td>\n",
              "      <td>0.388</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>7</td>\n",
              "      <td>196</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39.8</td>\n",
              "      <td>0.451</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>9</td>\n",
              "      <td>119</td>\n",
              "      <td>80</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.263</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>11</td>\n",
              "      <td>143</td>\n",
              "      <td>94</td>\n",
              "      <td>33</td>\n",
              "      <td>146</td>\n",
              "      <td>36.6</td>\n",
              "      <td>0.254</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>10</td>\n",
              "      <td>125</td>\n",
              "      <td>70</td>\n",
              "      <td>26</td>\n",
              "      <td>115</td>\n",
              "      <td>31.1</td>\n",
              "      <td>0.205</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>7</td>\n",
              "      <td>147</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39.4</td>\n",
              "      <td>0.257</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>97</td>\n",
              "      <td>66</td>\n",
              "      <td>15</td>\n",
              "      <td>140</td>\n",
              "      <td>23.2</td>\n",
              "      <td>0.487</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>13</td>\n",
              "      <td>145</td>\n",
              "      <td>82</td>\n",
              "      <td>19</td>\n",
              "      <td>110</td>\n",
              "      <td>22.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>5</td>\n",
              "      <td>117</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>34.1</td>\n",
              "      <td>0.337</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>2</td>\n",
              "      <td>99</td>\n",
              "      <td>60</td>\n",
              "      <td>17</td>\n",
              "      <td>160</td>\n",
              "      <td>36.6</td>\n",
              "      <td>0.453</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>1</td>\n",
              "      <td>102</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39.5</td>\n",
              "      <td>0.293</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>11</td>\n",
              "      <td>120</td>\n",
              "      <td>80</td>\n",
              "      <td>37</td>\n",
              "      <td>150</td>\n",
              "      <td>42.3</td>\n",
              "      <td>0.785</td>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>3</td>\n",
              "      <td>102</td>\n",
              "      <td>44</td>\n",
              "      <td>20</td>\n",
              "      <td>94</td>\n",
              "      <td>30.8</td>\n",
              "      <td>0.400</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>1</td>\n",
              "      <td>109</td>\n",
              "      <td>58</td>\n",
              "      <td>18</td>\n",
              "      <td>116</td>\n",
              "      <td>28.5</td>\n",
              "      <td>0.219</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>9</td>\n",
              "      <td>140</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32.7</td>\n",
              "      <td>0.734</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>13</td>\n",
              "      <td>153</td>\n",
              "      <td>88</td>\n",
              "      <td>37</td>\n",
              "      <td>140</td>\n",
              "      <td>40.6</td>\n",
              "      <td>1.174</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>12</td>\n",
              "      <td>100</td>\n",
              "      <td>84</td>\n",
              "      <td>33</td>\n",
              "      <td>105</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.488</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>1</td>\n",
              "      <td>147</td>\n",
              "      <td>94</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>49.3</td>\n",
              "      <td>0.358</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>1</td>\n",
              "      <td>81</td>\n",
              "      <td>74</td>\n",
              "      <td>41</td>\n",
              "      <td>57</td>\n",
              "      <td>46.3</td>\n",
              "      <td>1.096</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>3</td>\n",
              "      <td>187</td>\n",
              "      <td>70</td>\n",
              "      <td>22</td>\n",
              "      <td>200</td>\n",
              "      <td>36.4</td>\n",
              "      <td>0.408</td>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>6</td>\n",
              "      <td>162</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24.3</td>\n",
              "      <td>0.178</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>4</td>\n",
              "      <td>136</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31.2</td>\n",
              "      <td>1.182</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>1</td>\n",
              "      <td>121</td>\n",
              "      <td>78</td>\n",
              "      <td>39</td>\n",
              "      <td>74</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.261</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>752</th>\n",
              "      <td>3</td>\n",
              "      <td>108</td>\n",
              "      <td>62</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.223</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>753</th>\n",
              "      <td>0</td>\n",
              "      <td>181</td>\n",
              "      <td>88</td>\n",
              "      <td>44</td>\n",
              "      <td>510</td>\n",
              "      <td>43.3</td>\n",
              "      <td>0.222</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>8</td>\n",
              "      <td>154</td>\n",
              "      <td>78</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>32.4</td>\n",
              "      <td>0.443</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>1</td>\n",
              "      <td>128</td>\n",
              "      <td>88</td>\n",
              "      <td>39</td>\n",
              "      <td>110</td>\n",
              "      <td>36.5</td>\n",
              "      <td>1.057</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>90</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.391</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0.258</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>1</td>\n",
              "      <td>106</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.5</td>\n",
              "      <td>0.197</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>6</td>\n",
              "      <td>190</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.5</td>\n",
              "      <td>0.278</td>\n",
              "      <td>66</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>2</td>\n",
              "      <td>88</td>\n",
              "      <td>58</td>\n",
              "      <td>26</td>\n",
              "      <td>16</td>\n",
              "      <td>28.4</td>\n",
              "      <td>0.766</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>761</th>\n",
              "      <td>9</td>\n",
              "      <td>170</td>\n",
              "      <td>74</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.403</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>762</th>\n",
              "      <td>9</td>\n",
              "      <td>89</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22.5</td>\n",
              "      <td>0.142</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
              "0       6   148    72    35     0  33.6  0.627   50      1\n",
              "1       1    85    66    29     0  26.6  0.351   31      0\n",
              "2       8   183    64     0     0  23.3  0.672   32      1\n",
              "3       1    89    66    23    94  28.1  0.167   21      0\n",
              "4       0   137    40    35   168  43.1  2.288   33      1\n",
              "5       5   116    74     0     0  25.6  0.201   30      0\n",
              "6       3    78    50    32    88  31.0  0.248   26      1\n",
              "7      10   115     0     0     0  35.3  0.134   29      0\n",
              "8       2   197    70    45   543  30.5  0.158   53      1\n",
              "9       8   125    96     0     0   0.0  0.232   54      1\n",
              "10      4   110    92     0     0  37.6  0.191   30      0\n",
              "11     10   168    74     0     0  38.0  0.537   34      1\n",
              "12     10   139    80     0     0  27.1  1.441   57      0\n",
              "13      1   189    60    23   846  30.1  0.398   59      1\n",
              "14      5   166    72    19   175  25.8  0.587   51      1\n",
              "15      7   100     0     0     0  30.0  0.484   32      1\n",
              "16      0   118    84    47   230  45.8  0.551   31      1\n",
              "17      7   107    74     0     0  29.6  0.254   31      1\n",
              "18      1   103    30    38    83  43.3  0.183   33      0\n",
              "19      1   115    70    30    96  34.6  0.529   32      1\n",
              "20      3   126    88    41   235  39.3  0.704   27      0\n",
              "21      8    99    84     0     0  35.4  0.388   50      0\n",
              "22      7   196    90     0     0  39.8  0.451   41      1\n",
              "23      9   119    80    35     0  29.0  0.263   29      1\n",
              "24     11   143    94    33   146  36.6  0.254   51      1\n",
              "25     10   125    70    26   115  31.1  0.205   41      1\n",
              "26      7   147    76     0     0  39.4  0.257   43      1\n",
              "27      1    97    66    15   140  23.2  0.487   22      0\n",
              "28     13   145    82    19   110  22.2  0.245   57      0\n",
              "29      5   117    92     0     0  34.1  0.337   38      0\n",
              "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
              "738     2    99    60    17   160  36.6  0.453   21      0\n",
              "739     1   102    74     0     0  39.5  0.293   42      1\n",
              "740    11   120    80    37   150  42.3  0.785   48      1\n",
              "741     3   102    44    20    94  30.8  0.400   26      0\n",
              "742     1   109    58    18   116  28.5  0.219   22      0\n",
              "743     9   140    94     0     0  32.7  0.734   45      1\n",
              "744    13   153    88    37   140  40.6  1.174   39      0\n",
              "745    12   100    84    33   105  30.0  0.488   46      0\n",
              "746     1   147    94    41     0  49.3  0.358   27      1\n",
              "747     1    81    74    41    57  46.3  1.096   32      0\n",
              "748     3   187    70    22   200  36.4  0.408   36      1\n",
              "749     6   162    62     0     0  24.3  0.178   50      1\n",
              "750     4   136    70     0     0  31.2  1.182   22      1\n",
              "751     1   121    78    39    74  39.0  0.261   28      0\n",
              "752     3   108    62    24     0  26.0  0.223   25      0\n",
              "753     0   181    88    44   510  43.3  0.222   26      1\n",
              "754     8   154    78    32     0  32.4  0.443   45      1\n",
              "755     1   128    88    39   110  36.5  1.057   37      1\n",
              "756     7   137    90    41     0  32.0  0.391   39      0\n",
              "757     0   123    72     0     0  36.3  0.258   52      1\n",
              "758     1   106    76     0     0  37.5  0.197   26      0\n",
              "759     6   190    92     0     0  35.5  0.278   66      1\n",
              "760     2    88    58    26    16  28.4  0.766   22      0\n",
              "761     9   170    74    31     0  44.0  0.403   43      1\n",
              "762     9    89    62     0     0  22.5  0.142   33      0\n",
              "763    10   101    76    48   180  32.9  0.171   63      0\n",
              "764     2   122    70    27     0  36.8  0.340   27      0\n",
              "765     5   121    72    23   112  26.2  0.245   30      0\n",
              "766     1   126    60     0     0  30.1  0.349   47      1\n",
              "767     1    93    70    31     0  30.4  0.315   23      0\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "kkQEXq8pWIE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's clearly define which columns will act as explanatory variables, and which column will be the target value, and split the dataset between your training data and testing data. Let's try an 80-20 split.\n"
      ]
    },
    {
      "metadata": {
        "id": "JiFneXYtWGxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0wSn5KkhWWPg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now start building our predict function. Our predict function will have four parameters: a training and testing dataset for explanatory variables, a training dataset for the target variable, and some number of neighbors \"k\".\n",
        "\n",
        "But before we get ahead of ourselves, let's compute the euclidean distance between the “new” observation and all the data points in the training set, and have it sorted, too. (Hint: use the one you just made!)"
      ]
    },
    {
      "metadata": {
        "id": "2Ux-Fc-mWV7_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(x_training,y_training,x_testing,k):\n",
        "  distances=[]\n",
        "  ## Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_HrEdJmOXuYd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In our now sorted list of distances, it makes it easier to grab the K nearest neighbors (first K distances), and get their associated labels, which we should store in a targets array."
      ]
    },
    {
      "metadata": {
        "id": "hy_QrkpBXumm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "targets=[]\n",
        "## Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uPX4y-BgZaue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the function Counter from the module collections, we can now find the most common target among the values in our target array."
      ]
    },
    {
      "metadata": {
        "id": "6aYTwj-gZa0e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "c=Counter(targets)\n",
        "return c.most_common()[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDzW-QSyaeKV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's make a knn function that  loops over all the observations in our testing data and creates predictions."
      ]
    },
    {
      "metadata": {
        "id": "ze3gUvccad1O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def knn(x_training, y_training, x_testing, predictions, k):\n",
        "\t## Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90G8MqMcbFXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now equipped with our predicted values and actual values for the target variables,  all that's left to do is to create a metric to quantify how well the knn model did. We can do this in terms of accuracy, which is: the number of correctly predicted values over the total number of predictions."
      ]
    },
    {
      "metadata": {
        "id": "ereqdj33bFdl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(x_testing,predictions):\n",
        "  ##Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2XqMd6UZGrm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using sklearn to speed up normalizing and distance finding\n",
        "\n",
        "Luckily for us, sklearn has some quick and easy functions for normalizing the data, finding Euclidean distances, training, and testing with k-NNA. Try k = 5 to start."
      ]
    },
    {
      "metadata": {
        "id": "hCvykN8WYX3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# columns we will use to make predictions with\n",
        "x_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "\n",
        "# column that we want to predict\n",
        "y_col = ['class']\n",
        "\n",
        "# Create Model with k nearest neighbors\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "# Train/fit model with training data\n",
        "knn.fit(data[x_cols], data[y_col])\n",
        "\n",
        "# Make predictions on the test data using the fitted model\n",
        "predictions = knn.predict(data[x_cols])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JEXhZK9qYZbg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computing Error\n",
        "\n",
        "Now that we have our predictions, we can see how close they are to the real data. A simple and common way to compute error is with mean square error (MSE). In the MSE formula below, n is the number of data points, Yi is the predicted value, and Yhat is the truth value for the data point i.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*20m_U-H6EIcxlN2k07Z7oQ.png)\n",
        "\n",
        "In the cell below, calculate MSE for our predictions."
      ]
    },
    {
      "metadata": {
        "id": "o5MdYPD7Ybtv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sRGS2qDEYd6d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should get MSE = 0.133958 points. What does this number mean? It literally means that the mean of the raw error for each class prediction squared is 0.133958. For this number to be useful, we need to be able to compare it to other MSE measurements from a prediction set generated by a different model.\n",
        "\n",
        "Spend a few minutes trying to lower the MSE as much as you can by changing k and the features of the data you are using to predict values.\n",
        "\n",
        "* What set of features and values of k did you find to be the most optimal?\n",
        "* Why is choosing the right features so important for prediction accuracy? \n",
        "* How might we use non-numerical data columns in our model (if we had any)?"
      ]
    },
    {
      "metadata": {
        "id": "BWt8cLRuYfq_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pros and Cons of k-NNA\n",
        "\n",
        "## Pros\n",
        "* Non-parametric (can be used with data that does not fit a normal distribution)\n",
        "* Conceptually simple and relatively simple to instantiate\n",
        "* Little to no \"training\" time (unlike neural networks)\n",
        "* A good starting point/baseline classifier \n",
        "\n",
        "## Cons\n",
        "* Slow \"testing\" phase compared to other predictors/classifiers \n",
        "* Degrades with high-dimension data (because there is less difference between closest and furthest neighbors)\n",
        "* Doesn't handle data with skewed class distribution well (if one class is extremely dominant in the training data, it will dominate the \"voting majority\" for classifying new data)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "From these exercises and the pros and cons listed above, here is when it is most useful to use k-NNA:\n",
        "* Datasets with many data points and few dimensions (but can become very slow as well)\n",
        "* Datasets that are non-parametric\n",
        "* When you want a quick and easy classifier that does not have to be optimal (perhaps to use as a baseline for other models)\n",
        "\n",
        "\n",
        "Lastly, there are many other types of NNAs and there are many other ways to instantiate the k-NNA. For example, we could have used a weighted voting system (nearer neighbors vote's carry more weight) instead of a majority voting system to classify. Additionally, we could have used a different distance measurement or a different error measurement. "
      ]
    },
    {
      "metadata": {
        "id": "NXkFov8BWBIM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}